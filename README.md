# Warp & eTomo Cryo-ET Pipeline

A robust and flexible pipeline designed to streamline cryo-electron tomography (cryo-ET) data processing by integrating the capabilities of **Warp** and **IMOD/eTomo**. This pipeline automates the workflow from raw data import to final tomogram reconstruction and optimization.

## Table of Contents

- [About The Project](#about-the-project)
  - [Key Features](#key-features)
  - [Workflow Diagram](#workflow-diagram)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [How to Run](#how-to-run)
  - [1. Configure the Pipeline](#1-configure-the-pipeline)
  - [2. Execute the Wrapper Script](#2-execute-the-wrapper-script)
  - [3. Understanding the "Snapshot" Mechanism](#3-understanding-the-snapshot-mechanism-important)
- [Configuration Guide](#configuration-guide)
  - [General Settings](#general-settings)
  - [Key Acquisition Parameters](#key-acquisition-parameters)
  - [Camera-Specific Settings](#camera-specific-settings)
  - [Computing Resources](#computing-resources)
  - [Derived Parameters (Do Not Edit)](#derived-parameters-do-not-edit)
- [Pipeline Stages Explained](#pipeline-stages-explained)
  - [Stage 1: `preprocess`](#stage-1-preprocess)
  - [Stage 2: `etomo`](#stage-2-etomo)
  - [Stage 3: `optimize`](#stage-3-optimize)
  - [Stage 4: `postprocess`](#stage-4-postprocess)
- [Appendix: Additional Processing Modules (`run_appendix.py`)](#appendix-additional-processing-modules-run_appendixpy)
  - [`isonet`](#isonet)
  - [`cryolo`](#cryolo)
  - [`3DTM`](#3dtm)
  - [`reconstruction`](#reconstruction)
- [Logging System](#logging-system)
- [License](#license)
- [Contact](#contact)
- [Acknowledgements](#acknowledgements)

## About The Project

This project provides a powerful, automated pipeline for cryo-ET data processing. It handles the entire workflow, from organizing raw data to optimizing tilt-series alignment with a custom eTomo routine. The code has been carefully refactored to be modular, robust, and easy to maintain.

### Key Features

*   **Multi-Camera Support**: Seamlessly processes data from both K3 (.tif) and Falcon4 (.eer) cameras.
*   **Automated Workflow**: Integrates Warp and a custom eTomo optimization routine to streamline processing.
*   **Centralized Configuration**: Manages all parameters in a single `config.py` file.
*   **Advanced eTomo Optimization**: A parallelized stage identifies and excludes high-error views and fiducials to improve alignment.
*   **Flexible Gain Handling**: Automatically finds the gain reference if not explicitly configured.
*   **Structured Logging**: Generates organized logs for easy tracking and debugging.

### Workflow Diagram

The pipeline is organized into four distinct stages that can be run sequentially or individually.

```
+----------------------+
|   Input: Raw Data    |
| (.tif/.eer, .mdoc)   |
+----------------------+
           |
           v
+----------------------+      [1. Prepares data, runs motion correction & CTF]
|   Stage: preprocess  |
+----------------------+
           |
           v
+----------------------+      [2. Runs initial eTomo alignment via Warp]
|     Stage: etomo     |
+----------------------+
           |
           v
+----------------------+      [3. Custom eTomo optimization loop]
|    Stage: optimize   |-----> - Analyzes align.log for outliers
+----------------------+      - Prunes fiducial model
           |                  - Re-runs alignment and reconstruction
           v
+----------------------+      [4. Imports final alignment, performs deconvolution]
|  Stage: postprocess  |
+----------------------+
           |
           v
+----------------------+
|  Output: Optimized   |
|      Tomograms       |
+----------------------+
```

## Getting Started

Follow these steps to get a local copy up and running.

### Prerequisites

*   A Linux environment with support for `module` commands.
*   **Python 3.x**
*   **IMOD**: [IMOD Website](https://bio3d.colorado.edu/imod/)
*   **Warp**: [Warp Website](https://www.warpem.com/)

### Installation

1.  **Clone the repository:**
    ```sh
    git clone https://github.com/Siyu-C-TOMO/Warp_Pipeline.git
    ```
2.  **Navigate into the directory:**
    ```sh
    cd Warp_Pipeline
    ```
3.  **Install required Python packages:**
    ```sh
    pip install -r requirements.txt
    ```
4.  **Make the wrapper script executable** (only needs to be done once):
    ```sh
    chmod +x warp_wrapper.sh
    ```

## How to Run

The main entry point is the `warp_wrapper.sh` script. It ensures the correct environment is loaded before executing the Python pipeline.

### 1. Configure the Pipeline

Edit the `config.py` located in the `Warp_Pipeline` source directory.
* **For new projects**: This file acts as a **template**. Set your desired parameters here before the first run.
* **For existing projects**: Do NOT edit this file. Edit the local config inside your project folder instead.

### 2. Execute the Wrapper Script

Run the script from the parent directory where you want your dataset folder to be created.

```sh
# Example: Running from your workspace
cd /data/workspace/Siyu/Titan1_Processing

# Run the pipeline
/path/to/Warp_Pipeline/warp_wrapper.sh --stage preprocess

**Command-Line Arguments:**

*   `--stage`: Specifies which part of the pipeline to run.
    *   `all`: (Default) Runs the full pipeline from start to finish.
    *   `preprocess`: Runs only the preprocessing stage.
    *   `etomo`: Runs only the initial eTomo alignment stage.
    *   `optimize`: Runs only the custom eTomo optimization stage.
    *   `postprocess`: Runs only the final post-processing stage.

**Examples:**

```sh
# Run the full pipeline
./path/to/Warp_Pipeline/warp_wrapper.sh --stage all

# Run only the custom optimization and post-processing
./path/to/Warp_Pipeline/warp_wrapper.sh --stage optimize
./path/to/Warp_Pipeline/warp_wrapper.sh --stage postprocess
```

### 3. Understanding the "Snapshot" Mechanism (Important!)
To ensure reproducibility, this pipeline uses a Configuration Snapshot system:

**First Run:**
- The script reads dataset_name from the global config.py.
- It creates the project folder (e.g., 20250820_HSC/).
- It COPIES the global config.py into 20250820_HSC/config.py.
- It runs the pipeline using this local snapshot.

**Subsequent Runs / Resuming:**
- The script detects 20250820_HSC/config.py already exists.
- It ignores the global config and loads the local snapshot.

⚠️ **CRITICAL WARNING:** If you need to change parameters (e.g., angpix, dose) for an ongoing project, you must edit the config.py INSIDE your dataset folder. Modifying the global template will have no effect on existing runs.

## Configuration Guide

All user-editable settings are located at the top of `config.py`.
> **Note**: This guide describes the parameters. Remember that for an active run, you are editing the `config.py` **inside your dataset directory**.

### General Settings

*   `dataset_name`: A unique name for your processing run.
    *   **For K3 data**, this should match the name of the raw data folder.
    *   **For Falcon4 data**, you can define a new name for the reorganized dataset.
*   `raw_directory`: The root path where your raw data is, or will be, stored.
    *   **For K3 data**, this is the directory containing the `dataset_name` folder.
    *   **For Falcon4 data**, this is the target directory where the reorganized data will be moved.
*   `Data Structure & Identifiers`:
    *   `frame_folder`: Subfolder for raw frames (e.g., `"frames"`).
    *   `mdoc_folder`: Subfolder for `.mdoc` files (e.g., `"mdocs"`).
*   `gain_ref`: Filename of the gain reference. If the specified file isn't found, the pipeline will automatically use any other `.gain` file present in the `frame_folder`.
*   `tomo_match_string`: A string to identify tomograms by filename (e.g., `"sq"`).

### Key Acquisition Parameters

*   `angpix`: Angstroms per pixel.
*   `dose`: Total exposure dose per tilt for the tilt-series.
*   `tilt_axis_angle`: The rotation angle of the tilt-axis.
*   `thickness_pxl`: Estimated tomogram thickness in pixels.
*   `camera_type`: Switch between `"K3"` or `"Falcon4"`.

### Camera-Specific Settings

*   **For Falcon4:**
    *   `falcon4_source_dir`: The source directory of your raw Falcon4 data.
        > **:warning: CRITICAL:** The contents of this directory will be **MOVED**, not copied, to the `raw_directory` location for processing. **NO BACKUP IS KEPT** in the original location. It is strongly not suggested to make any further changes in the `raw_directory` folder because the raw data is saved and re-organized there.
    *   `falcon4_eer_ngroups`: The number of groups to split each `.eer` file into for motion correction.
*   **For K3:**
    *   `k3_frame_num`: The number of frames per tilt in your K3 data.

### Computing Resources

*   `gpu_devices`: A list of GPU device IDs to use (e.g., `[0, 1]`).
*   `jobs_per_gpu`: Number of parallel jobs to run on each specified GPU.
*   `etomo_cpu_cores`: Number of CPU cores to use for the parallel eTomo optimization stage.

### Derived Parameters (Do Not Edit)

The section below the user settings in `config.py` contains parameters that are automatically calculated based on your inputs. **Do not modify this section unless you completely understand what you are doing.**

## Pipeline Stages Explained

### Stage 1: `preprocess`

**Goal**: Prepare data for alignment.
*   **Key Steps**:
    1.  If `camera_type` is "Falcon4", it first runs `reorganize_falcon4_data` to move and structure the data.
    2.  Creates symbolic links to raw frames, `.mdoc` files, and the gain reference in the processing directory.
    3.  Generates `warp_frameseries.settings` and `warp_tiltseries.settings` using `WarpTools create_settings`.
    4.  Runs frame-level motion correction and CTF estimation (`WarpTools fs_motion_and_ctf`).
    5.  Imports tilt-series metadata from `.mdoc` files to create the initial `tomostar` files (`WarpTools ts_import`).

### Stage 2: `etomo`

**Goal**: Perform initial tilt-series alignment using eTomo within Warp.
*   **Key Steps**:
    1.  Calculates the optimal patch size for alignment, either dynamically or using a fixed default.
    2.  Runs `WarpTools ts_etomo_patches` to perform alignment. This step uses custom IMOD wrappers to ensure compatibility.

### Stage 3: `optimize`

**Goal**: Refine the eTomo alignment by removing high-error views and fiducials.
*   **Key Steps**:
    1.  The `etomo_optimize.py` script is executed in parallel across all tomograms.
    2.  For each tomogram, it parses the `align.log` to identify views and fiducial contours with high residual errors (based on standard deviation).
    3.  It creates a new, pruned fiducial model (`.fid` file) that excludes the identified outliers.
    4.  It generates cleaned `.com` files (`align_clean.com`, `tilt_clean.com`) with an `ExcludeList` parameter.
    5.  It re-runs the alignment (`submfg align_clean.com`) and tomogram reconstruction (`submfg tilt_clean.com`).

### Stage 4: `postprocess`

**Goal**: Finalize the tomograms.
*   **Key Steps**:
    1.  Imports the newly optimized alignments from the `optimize` stage back into Warp (`WarpTools ts_import_alignments`).
    2.  Checks and corrects for defocus handedness issues.
    3.  Estimates the tilt-series CTF (`WarpTools ts_ctf`).
    4.  Parses the final `align_clean.com` files to update Warp's `.xml` files, ensuring bad tilts are excluded from the final reconstruction.
    5.  Performs deconvolution on the final reconstructed tomograms using IMOD's `reducefiltvol`.

## Appendix: Additional Processing Modules (`run_appendix.py`)

The `run_appendix.py` script provides a collection of standalone modules for downstream processing tasks that are not part of the main alignment pipeline. These tools can be run independently after the main pipeline has generated the necessary tomograms and metadata.

### How to Run Appendix Modules

Each module is executed by specifying its name via the `--stage` argument. Unlike the main pipeline, this script is a standard Python script and does not require the `warp_wrapper.sh`.

```sh
# General command structure
python /path/to/Warp_Pipeline/run_appendix.py --stage <module_name>
```

**Available Modules:**

*   `isonet`: Tomogram denoising using ISONet.
*   `cryolo`: Particle picking with CryoLo and particle extraction.
*   `3DTM`: 3D template matching using Warp.
*   `reconstruction`: Final reconstruction and packaging for Windows.

### Module Details

#### `isonet`
**Goal**: Denoise tomograms using the ISONet deep learning model.
*   **Key Steps**:
    1.  Reads a list of tomograms from `ribo_list_final.txt`.
    2.  Creates symbolic links to the tomograms in a dedicated `isonet/tomoset` directory.
    3.  Executes the ISONet training, prediction, and post-processing steps as defined in the configuration.

#### `cryolo`
**Goal**: Perform automated particle picking using a pre-trained CryoLo model.
*   **Key Steps**:
    1.  If not exist, prepare a `cryolo` directory with a star-handler command.
    2.  Runs CryoLo prediction on the tomograms listed in `ribo_list_final.txt`.
    3.  Converts the output coordinates (`.coords`) to `.star` files.
    4.  Filters the picked particles based on a Z-coordinate range specified in `ribo_list_final.txt` to exclude particles near the top or bottom of the tomogram.
    5.  Exports the final particle stacks using `WarpTools ts_export_particles`.

#### `3DTM`
**Goal**: Perform 3D template matching to locate macromolecules.
*   **Key Steps**:
    1.  Uses a template specified in `config.py`.
    2.  Reads a list of tomograms to process.
    3.  Runs `WarpTools ts_template_match` to perform the search and generate cross-correlation maps and peak files.

#### `reconstruction`
**Goal**: Generate a final reconstruction and package key files for easy transfer to a Windows machine.
*   **Key Steps**:
    1.  Runs `WarpTools ts_reconstruct` to generate the final tomogram.
    2.  Creates a `forWindows_frames` directory.
    3.  Creates symbolic links in this directory to the final reconstruction, average frame, `.xml` metadata, and `.tomostar` files, making it easy to archive or move the essential results.

## Logging System

The pipeline generates a structured set of logs inside your processing directory (`dataset_name`).

*   `logs/pipeline.log`: The main log file for the entire pipeline run.
*   `logs/<stage_name>.log`: A summary log for each major stage (e.g., `etomo_optimization.log`, `xml_parsing.log`).
*   `logs/tomograms/<tomo_name>/`: A dedicated directory for each tomogram, containing detailed logs for specific processing steps, most notably the `optimization.log` from the custom refinement stage.

## License

Distributed under the MIT License.

## Contact

Siyu Chen - sic027@ucsd.edu

Project Link: [https://github.com/Siyu-C-TOMO/Warp_Pipeline](https://github.com/Siyu-C-TOMO/Warp_Pipeline)

## Acknowledgements

*   Thanks to Joshua Hutchings for the original idea and initial code implementation for the alignment optimization part of this pipeline.
